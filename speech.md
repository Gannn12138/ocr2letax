
### **面向大规模MoE模型和行业模型的国产化部署方案探索——演讲稿**

**(开场)**

**主持人/演讲者 杨泽乾：**

各位来宾，各位专家，下午好。

我是来自清昴智能的杨泽乾，非常荣幸能站在这里，与大家共同探讨一个当前AI领域非常核心且具有挑战性的话题：面向大规模MoE（Mixture-of-Experts）模型和行业专用模型的国产化部署与优化。

---

**(第2页：清昴智能介绍)**

**杨泽乾：**

在正式开始之前，请允许我简单介绍一下我们公司——清昴智能。

简单来说，清昴智能是一家AI基础设施（AI Infra）供应商，我们的核心使命是解决AI落地过程中的“最后一公里”问题。具体来说，我们专注于两大领域：一是国产芯片的深度适配，二是AI模型的自动化优化。我们致力于解决当前企业在AI转型中遇到的普遍痛点，比如复杂AI模型落地难、实际运行性能差、计算资源耗费高，以及在国产化替代趋势下，模型到硬件的适配难题。

我们的创始团队源自清华大学计算机系，核心成员汇聚了来自清华、上海交大、斯坦福等海内外顶尖高校，以及华为、阿里、AMD等知名科技企业的优秀人才。公司成立三年以来，我们非常荣幸地获得了华为哈勃的战略投资，以及多家国内一线基金累计上亿元的财务投资，这既是对我们技术实力的认可，也是对我们发展方向的肯定。

目前，我们已经推出了一系列成熟的产品，包括面向华为昇腾全系列产品线的大模型推理部署工具集、能够统一管理和调度国产异构算力的智算云产品，以及像DeepSeek一体机这样的软硬结合解决方案。我们希望通过这些产品，填补大模型高效落地这一领域的空白。作为华为昇腾的重要合作伙伴，我们的最终目标是赋能千行百业，帮助企业客户真正、高效地完成智能化升级。

---

**(第3-6页：大规模MoE模型部署优化探索)**

**杨泽乾：**

好，让我们正式进入今天的主题：大规模MoE模型的部署优化探索。

首先，我们回顾一下大模型基座的技术路线变迁。AI的发展浪潮可谓一波接一波。从最初大家熟知的**Dense Transformer**架构，到如今为了追求更高效率和更大参数量而复兴的**稀疏化与MoE架构**，我们看到了模型设计思想的演进。同时，业界也在积极探索可能**超越Transformer**的新架构。未来的趋势很可能是多种架构并存的**混合架构**。

在这种演进中，一个非常重要的范式变化是**“Scaling Law”**，也就是“规模定律”的关注点发生了转移。

请看这张表格。在过去，我们谈论Scaling Law，更多是指**训练阶段（Training Scaling）**，大家关心的是用更多的数据、更大的模型、更多的计算量，能换来多低的loss。但现在，随着模型能力的提升，其边际效益已经逐渐显现。

而如今，业界的焦点正转向**推理阶段（Inference Scaling）**。我们面临的新问题是：如何让模型在推理时，能像人一样自动构建思维过程，将一个复杂的任务分解和规划？以及，在处理日益复杂的任务时，如何极致地提升模型的推理效率？

这给我们带来了两大核心挑战。第一是**上下文工程（Context Engineering）**。现在我们不再仅仅依赖模型的基础能力，而是通过设计精巧、信息丰富的上下文来引导和控制模型的输出，从而大幅提升其在特定任务上的准确率。第二，就是我们今天重点讨论的**MoE架构**。它通过在推理时只激活一部分专家（Experts），极大地降低了实际计算的参数量，是实现高效“Inference Scaling”的关键技术。

总而言之，行业AI的落地范式正在发生深刻变化。我们正从过去单纯依赖模型本身，转向一种“**模型 + 上下文工程 + MCP（可以理解为AGENT模式）**”的新范式。在这个新范式下，上下文工程的复杂度、信息密度和质量，变得前所未有的重要。

这里有一个很有说服力的数据：自从Anthropic公司在五月份推出其Claude 4模型后，其面向代码场景的Claude Code，每周下载量暴增6倍，达到300万次；活跃用户增长了300%，收入增长更是超过5.5倍。 这背后，正是强大的模型基座与复杂的上下文工程（Agent模式）结合所带来的巨大成功。

---

**(第7-11页：从模型基座到上下文工程 & 国产化挑战)**

**杨泽乾：**

那么，当我们进入这个“上下文工程”或者说“Agent”时代，模型在推理时会呈现出哪些显著的特点呢？

首先是**长上下文与长输出**。我们分析了一个典型的前后端开发项目，发现Agent在与用户交互时，平均单次处理的总上下文长度在16k到20k tokens，而生成的代码或文本输出也达到了2k到4k tokens。

其次是**频繁的工具调用**。像我们刚刚提到的Claude Code，在运行时会频繁调用各种工具插件，比如命令行（bash）、文件操作（file）等等，甚至会调用领域定制的子Agent（subagent）来协同工作。平均每次对话，可能会触发大约10次左右的插件调用。

最后一点，也是非常关键的一点，是**高度重合的输入信息**。一个设计精良的Agent系统，其系统提示词（System Prompt）本身可能就长达4k tokens，这部分内容在每一轮对话中都是重复的。我们统计发现，在多轮对话中，输入的token重合率高达70%。

这些特点——长上下文、高频调用、大量重复输入——给我们的推理系统，特别是部署在国产化芯片上的系统，带来了独特的挑战。

我们以典型的国产芯片昇腾NPU为例，来对比一下它和主流GPU的特性差异。

大家可以看到这张表，NPU在**显存容量**上拥有巨大优势，单卡可以达到96G，是GPU的4倍，这对于加载动辄上千亿参数的大模型非常有利。但同时，它的**显存带宽**和**互联带宽**相比GPU存在明显差距，大约只有GPU的1/3到1/2。此外，在**精度支持**上，NPU原生不支持像BF16和FP8这样的低精度格式，这给模型量化和优化带来了限制。

这些硬件上的差异，要求我们必须进行深度、细致的软件优化，包括**并行与通信优化、缓存与调度优化，以及算子层面的融合与优化**。只有这样，才能在国产硬件上真正发挥出大模型的强大能力。

---

**(第12-17页：MoE模型的国产化部署优化实践 - 量化)**

**杨泽乾：**

接下来，我将分享我们在MoE模型国产化部署方面的一些具体实践。我们首先从**模型量化**谈起。

模型量化是降低模型部署成本最直接有效的手段，但在MoE模型和国产NPU的组合下，我们遇到了三大挑战：

1. **模型参数量巨大，常规量化环境无法支持**。近期开源的Kimi K2模型，参数量高达1万亿（1T）。如果使用BF16精度加载，需要大约2.2T的显存，没有任何单台机器能满足这个要求。因此，量化过程无法在纯粹的GPU或NPU环境中完成。
2. **硬件架构限制，量化方案受限**。如前所述，NPU不支持FP8，同时其计算核心（Cube Core）和向量核心（Vector Core）之间的缓存有限，这导致一些需要频繁数据交互的动态量化方案性能不佳。比如，Reciprocal算子在NPU上的精度和性能都不理想。
3. **FP16精度敏感**。NPU不支持BF16，只能用FP16。但我们发现，对于像Deepseek这样的模型，如果直接将hidden state用FP16来表示，会引发明显的精度丢失问题。

为了解决这些问题，我们设计了一套**异构量化系统**。这个系统利用**DISK-CPU-GPU三级缓存机制**，将4T以上的硬盘、256GB的内存和64GB的显存协同工作，通过Prefetch和Offload策略，动态地将模型参数在三者之间调度，从而避免了显存或内存的溢出。同时，我们采用流水线并行（Pipeline Parallelism）的方式，最大化地利用有限的GPU/NPU资源，并对模型进行分层量化，避免内存颠簸。

在具体的量化算法上，针对硬件的局限性，我们选择了一种**组合静态量化**的方案。我们避免使用性能不佳的reciprocal算子。具体来说，对权重（weight）采用per-channel对称量化，对激活值（activation）采用per-tensor对称量化，这样可以最大程度地减少计算核心之间的数据传输。为了弥补静态量化可能带来的精度损失，我们采用了**Smooth Quant + GPTQ**的组合策略。Smooth Quant技术可以平滑激活值的分布，让模型更容易被量化，而GPTQ则可以进一步通过优化来恢复权重的量化损失。

即便如此，我们还发现了一个更深层次的问题。在使用纯FP16精度的Deepseek V3模型时，虽然它在常规评测集上精度正常，但在实际生成文本时，偶尔会出现某些特殊token反复出现的现象，这是典型的精度丢失表现。通过逐层敏感性分析，我们最终定位到问题出在最后一层的MoE线性层（Linear Layer）。它对计算精度异常敏感。我们的解决方案是进行**精度感知的混合量化**：保持模型绝大部分为FP16，但将最后一层的MoE部分保留为FP32进行计算。调整后，这个现象就完全消失了。

大家可以看屏幕上的这两个生成示例，即使在复杂的逻辑推理和中英文混合的场景下，模型的输出依然准确、流畅，这证明了我们量化方案的成功。

---

**(第18-20页：长文本优化)**

**杨泽乾：**

解决了模型大小的问题，我们再来看Agent时代带来的另一个巨大挑战：**长文本推理**。

一个20k上下文长度的请求，其KV Cache在单batch下就需要占用约1.5G的显存。如果考虑到张量并行（TP）等需要复制缓存的并行策略，这个数字还会成倍增加。

为了减少KV Cache带来的存储和计算压力，一个常用的方法是**缓存压缩**，也就是只保留那些“重要”token的KV Cache。但现有的压缩算法，往往只从理论层面计算token的重要性，选出来的token在内存中是分散、不连续的，这对于硬件执行非常不友好，无法发挥出硬件的最高性能。

我们的核心思想是，**在进行稀疏优化时，必须考虑硬件的特性**。我们观察到token具有局部性（localization）特征，因此，我们提出了一种**硬件友好的KV Cache稀疏方案**。我们的方法不是以单个token为单位，而是**以block（块）为单位进行稀疏化操作**，这样可以保证内存访问的连续性。我们的流程分为四步：首先计算每个token的重要性，然后将token组织成块，接着对块的重要性进行聚合，最后选择最重要的块保留下来。

效果如何呢？请看这张表。在一个16k上下文长度、保留2k预算的测试场景下，基线方案（Baseline）的解码延迟是258毫秒。传统的稀疏方案（Ordinal）能优化到86毫秒，实现了3倍加速。而我们的方案（Ours），则将延迟进一步降低到了40毫秒，实现了惊人的**6.5倍加速**！这充分证明了硬件友好设计的重要性。

---

**(第21-23页：并行方案优化)**

**杨泽乾：**

处理大规模模型，单卡性能再强也是不够的，必须依赖多卡并行。但在国产芯片上，并行方案的设计面临着独特的局限性。

首先，**芯片间互联带宽有限**。以300I DUO卡为例，其芯片间的All Reduce性能大约只有同规格4090显卡的1/3到1/2。而且，它的通信单元数量有限，官方通信库甚至无法支持超过4卡的通信。

其次，**通信拓扑是异构的**。300I DUO是单卡双芯设计，卡内两个芯片间通信速度很快，但跨卡的芯片间通信速度就慢了很多。在一台16卡的服务器中，还存在跨PCIe交换机、甚至跨CPU的更复杂的通信情况。

在这种情况下，业界已有的并行方案，比如vLLM和SGLang默认使用的张量并行（TP），由于需要大量的全局通信，对带宽要求极高，因此效率不佳。DeepMind为Deepseek模型设计的专家并行（EP）方案，虽然能均衡负载，但同样需要全局通信，且存在木桶效应。

我们的解决方案是，设计一种**异构拓扑感知的并行策略**。我们首先开发了**层级通信策略**，针对不同的通信域（例如卡内、跨卡）使用不同的通信算法。例如，我们重写了AllReduce和AlltoAll通信算子，使其能够感知硬件拓扑，性能分别提升了2.21倍和1.84倍。

在此基础上，我们采用了**流水线并行（PP）和张量并行（TP）相结合**的混合并行策略。我们将计算任务划分到流水线的不同阶段，尽量将通信密集的TP限制在通信速度最快的单元内（例如单卡内的两个芯片），而跨越慢速链路的通信则尽可能减少。

请看这张性能对比表。传统的TP方案，总耗时约170毫秒，其中通信占了70毫秒。EP方案稍好，为120毫秒。而我们的PP+TP混合方案，将总耗时降低到了**85毫秒**，其中通信开销仅为25毫秒。虽然这种方案实现起来更复杂，扩展性稍差，但在特定硬件上，它能实现极致的性能。

---

**(第24-28页：缓存优化与算子优化)**

**杨泽乾：**

我们再回到缓存优化。前面提到，Agent模式下上下文重复率高达70%。如何高效利用这一特性？

SGLang论文中提出的**Radix Attention**是一个非常好的思路，它使用Radix Tree来存储和匹配具有相同前缀的请求，从而共享KV Cache。Radix Tree比普通的前缀树更高效，因为它的节点可以是变长的，能有效压缩树的深度和存储空间。

然而，Radix Tree本身的分裂、换出逻辑非常复杂，直接工程化实现难度很高。我们做了一项工程上的优化：**我们不以单个token作为树的节点，而是以之前提到的内存友好的“block”作为树节点**。这样既保留了Radix Tree高效匹配前缀的优势，又大大简化了工程实现的复杂度，使其更加鲁棒和高效。

当然，光有缓存技术还不够，还需要一个聪明的调度器。我们设计了**负载与缓存感知的调度策略**，它能实时感知系统中有哪些前缀被缓存了，以及各个计算节点的负载情况，动态地将新来的请求调度到最合适的节点上，从而实现负载均衡和缓存命中率的最大化。

最后，在最底层的算子层面，我们也进行了大量艰苦的优化工作。针对MoE模型中的关键算子，如**MOE Gate、MQA（Multi-Query Attention）和Dispatch**等，我们都进行了重写和深度优化。成果是显著的：优化后的MQA算子，性能提升了**41.2倍**；MoEGate算子性能提升了**21.5倍**；TokenExpertDispatcher算子也提升了**7.2倍**。这些底层算子的极致优化，是整个系统高性能的基石。

---

**(第29-34页：面向行业模型的国产化部署优化探索)**

**杨泽乾：**

到目前为止，我们主要讨论的是通用大语言模型。现在，让我们把目光转向另一个重要领域：**行业专用模型**，特别是**AI for Science（AI4S）**领域的模型。

在生命科学等领域，像**AlphaFold2、RoseTTAFold**这样的模型已经成为科研的利器，它们被广泛用于蛋白质结构预测、酶设计等场景。这些模型的核心算法，如Evoformer、SE(3)-等变注意力等，虽然强大，但也给国产化部署带来了巨大的挑战。

最大的痛点在于，这些模型中包含了**海量的自定义CUDA算子**。比如，一个FusedAttention算子，一个SE3Conv算子，在NPU上没有现成的高性能实现。我们只能选择手写CANN Kernel，或者更糟地，回退到CPU上执行。这直接导致模型推理的**吞吐量下降3到10倍**，完全无法满足科研或生产的需求。

我们总结了这类模型的共性架构特征：

1. **图 + 稀疏 + SE(3)等变**：模型中超过70%的计算都是不规则的稀疏图消息传递。但在NPU上，像 `index_add`这样的稀疏算子带宽利用率不到35%，远低于CUDA的优化水平。
2. **自定义算子链**：一次AlphaFold2的推理，可能要调用42个不同的CUDA Kernel。这使得适配工作量巨大。
3. **动态Shape + 大Batch**：蛋白质的长度是动态变化的，这会导致NPU的图编译缓存爆炸，子图数量可能超过10万个，严重影响编译效率。
4. **混合算子**：模型中稀疏算子和稠密算子混合出现，给编译器的统一调度带来了很大困难。

我们对推理流程进行瓶颈分析后发现，**稀疏计算阶段是绝对的瓶颈**。如果不能在底层的CANN层面重写高效的稀疏算子，那么上层再精妙的优化也无济于事。

因此，我们的解决方案是：**直面底层，硬磕算子**。我们使用华为提供的**AscendC**语言，从零开始开发了高性能的NPUScatter和NPUIndexAdd算子，彻底避免了到CPU的回退。在设计中，我们运用了多种内存优化策略，比如将随机访存操作，通过COO到CSC的格式预转换，变成了硬件更喜欢的连续访存，**带宽利用率从35%提升到了68%**。我们还利用双缓冲流水线来隐藏数据搬运的延迟，并充分利用TIK工具进行自动向量化。

最终的效果是，在一个512长度蛋白质的图消息传递阶段，**耗时从原来的620毫秒，骤降到了95毫秒**。这证明了底层算子优化的巨大价值。

---

**(第35-38页：总结与展望)**

**杨泽乾：**

我们今天分享的所有这些技术点——从异构量化、硬件友好的稀疏化，到拓扑感知的并行、负载感知的调度，再到底层稀疏算子的重写——最终都汇聚到了我们的核心产品**MLGuider**中，这是一个追求极致性能的异构推理引擎。

基于这个引擎，我们构建了更高层次的平台，包括高稳定性的算力调度平台和Agentic AI平台，从而能够为企业提供从底层模型部署到上层AI Agent应用开发的全栈式解决方案，端到端地解决企业在私有化部署AI时遇到的所有痛点。我们的理念，就是“**软件定义硬件**”，通过深度的软件优化，榨干每一分硬件的潜力。

清昴智能的使命，就是**打通AI落地的“最后一公里”**。我们致力于通过提供极致的性能、卓越的稳定性以及强大的跨平台能力，为我们的客户打造一个**全站自主可控的AI智能化业务底座**。

最后，屏幕上有我们的联系方式，包括公众号和合作联系方式。我们也在积极招聘，欢迎有志之士加入我们。

我的分享就到这里。谢谢大家！
